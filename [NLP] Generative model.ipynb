{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/masdevas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "import scrapy.crawler as crawler\n",
    "from scrapy.utils.log import configure_logging\n",
    "from multiprocessing import Process, Queue\n",
    "from twisted.internet import reactor\n",
    "from pydispatch import dispatcher\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets.arrow_dataset import Dataset\n",
    "# from transformers import AutoTokenizer\n",
    "# from transformers import AutoModelForSequenceClassification\n",
    "# from transformers import DistilGPT2Model\n",
    "# from transformers import DistilGPT2Tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2LMHeadModel\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import pipeline\n",
    "\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from lyricsgenius import Genius\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger('nlp')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_web_data(function):\n",
    "    def f(q):\n",
    "        try:\n",
    "            json_data = function()\n",
    "            q.put(json.dumps(json_data))\n",
    "        except Exception as e:\n",
    "            q.put(e)\n",
    "    q = Queue()\n",
    "    p = Process(target=f, args=(q,))\n",
    "    p.start()\n",
    "    result = q.get()\n",
    "    p.join()\n",
    "    gc.collect()\n",
    "    try:\n",
    "        json_data = json.loads(result)\n",
    "    except:\n",
    "        raise result\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USA government RSS channels parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_usagov_rss_data():\n",
    "    storage = {}\n",
    "    try:\n",
    "        class UsaGovSpider(scrapy.Spider):\n",
    "            name = \"usagov_checker\"\n",
    "            start_urls = ['https://www.state.gov/rss-feeds/']\n",
    "            \n",
    "            def parse(self, response, depth=0):\n",
    "                if depth == 0:\n",
    "                    a_selectors = response.xpath(\"//a\")\n",
    "                    for selector in a_selectors:\n",
    "                        link = selector.xpath(\"@href\").extract_first()\n",
    "                        if 'rss' in link:\n",
    "                            yield response.follow(link, self.parse, cb_kwargs={'depth' : 1})\n",
    "                else:\n",
    "                    storage[str(response.url)] = response.body.decode(\"utf-8\")\n",
    "\n",
    "        runner = crawler.CrawlerRunner()\n",
    "        deferred = runner.crawl(UsaGovSpider)\n",
    "        deferred.addBoth(lambda _: reactor.stop())\n",
    "        reactor.run()\n",
    "        return storage\n",
    "    except Exception as e:\n",
    "        return {'exception' : str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joke RSS channels parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_joke_rss_data():\n",
    "    storage = {}\n",
    "    try:\n",
    "        class JokeSpider(scrapy.Spider):\n",
    "            name = \"joke_checker\"\n",
    "            #start_urls = ['https://blog.feedspot.com/jokes_rss_feeds/'] # 403 error - bot detected, access denied\n",
    "            start_urls = [\n",
    "                'http://www.jokesoftheday.net/jokes-feed/', \n",
    "                'http://www.funnyshortjokes.com/feed',\n",
    "                'https://laffgaff.com/feed/',\n",
    "                'https://www.keeplaughingforever.com/blog//blog-feed.xml',\n",
    "                'https://lite92.ca/category/joke-of-the-day/feed/',\n",
    "                'https://www.thelaughline.com/feed/',\n",
    "                'https://www.jokesbykids.com/riddle/feed/',\n",
    "                'https://newbloggycat.com/category/good-clean-jokes/feed/',\n",
    "                'https://www.super-funny.com/feed/',\n",
    "                'http://slay.me/feed',\n",
    "                'https://www.funny-jokes-quotes-sayings.com/funny-jokes.xml',\n",
    "                'http://modest-jokes.blogspot.com/feeds/posts/default?alt=rss',\n",
    "                'https://badkidsjokes.tumblr.com/rss',\n",
    "                'http://chillyjokes.com/chillyjokes/jokes/feed/',\n",
    "                'https://laughbreak.com/pictures/its-okay-to-feed-the-ducks-bread-now/',\n",
    "                'https://acornyjokeaday.tumblr.com/rss',\n",
    "                'https://somejokeshere.blogspot.com/feeds/posts/default?alt=rss'\n",
    "            ]\n",
    "\n",
    "            def parse(self, response, depth=0):\n",
    "                storage[str(response.url)] = response.body.decode(\"utf-8\")\n",
    "                \n",
    "        runner = crawler.CrawlerRunner()\n",
    "        deferred = runner.crawl(JokeSpider)\n",
    "        deferred.addBoth(lambda _: reactor.stop())\n",
    "        reactor.run()\n",
    "        return storage\n",
    "    except Exception as e:\n",
    "        return {'exception' : str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rap texts parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_genius_token():\n",
    "    with open('genius_token.txt', 'r') as f:\n",
    "        token = f.readline().strip()\n",
    "    return token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rap_data():\n",
    "    genius = Genius(read_genius_token())\n",
    "    storage = {}\n",
    "    try:\n",
    "        class RapSpider(scrapy.Spider):\n",
    "            name = \"rap_checker\"\n",
    "            start_urls = [\n",
    "                'https://bestlifeonline.com/funniest-rap-lyrics/'\n",
    "            ]\n",
    "\n",
    "            def parse(self, response, depth=0):\n",
    "                resp = response.xpath(\"//div[@class='content noskimwords']\").xpath(\"//h2/text()\")\n",
    "                for item in resp:\n",
    "                    match = re.match(\"[0-9]+\\. ([A-Za-z0-9\\s\\-\\'\\.]+), \\\"([A-Za-z0-9\\s\\-\\'\\.]+)\\\"\", item.get().strip())\n",
    "                    if match:\n",
    "                        try:\n",
    "                            author = match.group(1)\n",
    "                            song = match.group(2)\n",
    "                            name = author+'|'+song\n",
    "                            print((author, song))\n",
    "                            song = genius.search_song(song, author)\n",
    "    #                         print(song.lyrics)\n",
    "                            print(len(song.lyrics))\n",
    "                            storage[name] = song.lyrics\n",
    "                        except Exception as e:\n",
    "                            print(f'!!! Exception passed on {name}', str(e))\n",
    "                \n",
    "        runner = crawler.CrawlerRunner()\n",
    "        deferred = runner.crawl(RapSpider)\n",
    "        deferred.addBoth(lambda _: reactor.stop())\n",
    "        reactor.run()\n",
    "        return storage\n",
    "    except Exception as e:\n",
    "        return {'exception' : str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING\n"
     ]
    }
   ],
   "source": [
    "storage_file = 'storage.pkl'\n",
    "def is_rawdata_exist(storage_file):\n",
    "    return os.path.exists(storage_file)\n",
    "    # return False\n",
    "\n",
    "if is_rawdata_exist(storage_file):\n",
    "    print('LOADING')\n",
    "    with open(storage_file, 'rb') as f:\n",
    "        storage = pickle.load(f)\n",
    "else:\n",
    "    print('FROM WEB')\n",
    "    storage = {}  \n",
    "    storage['usagov'] = get_web_data(get_usagov_rss_data)\n",
    "    storage['joke'] = get_web_data(get_joke_rss_data)\n",
    "    storage['rap'] = get_web_data(get_rap_data)\n",
    "    with open(storage_file, 'wb') as f:\n",
    "        pickle.dump(storage, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess sentences for HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/masdevas/.anaconda3/envs/nlp-unn/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "storage_to_process = copy.deepcopy(storage)\n",
    "\n",
    "map_labels = {'usagov' : 0, 'joke' : 1, 'rap' : 2}\n",
    "\n",
    "words_limit = 32\n",
    "words_at_least = 4\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    html_detector = re.compile(r'<.*?>')\n",
    "    return html_detector.sub(r'', text)\n",
    "\n",
    "def remove_links(text):\n",
    "    links_remover = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return links_remover.sub(r'', text)\n",
    "\n",
    "def split_sentence_to_approproate_length(res_proc_item):\n",
    "    sentences = []\n",
    "    splitted = res_proc_item.split(' ')\n",
    "    for idx in range(0, len(splitted), words_limit):\n",
    "        part_of_words = splitted[idx:idx+words_limit]\n",
    "        if len(part_of_words) < words_at_least:\n",
    "            continue\n",
    "        else:\n",
    "            sentences.append(' '.join(part_of_words))\n",
    "    return sentences\n",
    "\n",
    "def preprocess_html(body, local_processed_data, topic_key):\n",
    "#     l = 0\n",
    "    soup = BeautifulSoup(body)\n",
    "    p_tags = soup.find_all('p')\n",
    "    for each in p_tags:\n",
    "        processed = sent_tokenize(each.text.encode('ascii', 'ignore').decode('ascii').lower().strip())\n",
    "        for processed_item in processed:\n",
    "            processed_item = re.sub(\"[\\d\\.]+\", '', processed_item).strip()\n",
    "            if len(processed_item) > 0:\n",
    "                if topic_key == 'joke':\n",
    "                    if processed_item.startswith('a: ') or processed_item.startswith('q: '):\n",
    "                        res_proc_item = processed_item[3:]\n",
    "                    else:\n",
    "                        res_proc_item = processed_item\n",
    "                elif topic_key == 'usagov':\n",
    "                    if len(processed_item.split(' ')) > 3:\n",
    "                        res_proc_item = processed_item\n",
    "                else:\n",
    "                    raise Exception(f'Unknown topic_key for preprocess_html(): {topic_key}')\n",
    "#                 res_proc_item = ' '.join([:words_limit])\n",
    "                local_processed_data.extend(split_sentence_to_approproate_length(res_proc_item))\n",
    "#             l += len(processed)\n",
    "#     print(l)\n",
    "\n",
    "def get_data_rss(substorage, topic_key):\n",
    "    local_processed_data = []\n",
    "    for link, body in substorage.items():\n",
    "        preprocess_html(body, local_processed_data, topic_key)\n",
    "    local_processed_labels = [map_labels[topic_key]] * len(local_processed_data)\n",
    "    with open(f'tmp_{topic_key}.json', 'w') as f:\n",
    "        json.dump({'section' : local_processed_data}, f, indent=4, sort_keys=True)\n",
    "    return local_processed_data, local_processed_labels\n",
    "\n",
    "def join_rap_sentences(local_lines, lines_in_group):\n",
    "    rap_sentences = []\n",
    "    idx = 0\n",
    "    while idx < len(local_lines):\n",
    "        line = local_lines[idx].strip().lower()\n",
    "        if len(line) == 0:\n",
    "            del local_lines[idx]\n",
    "            continue\n",
    "        if idx + lines_in_group > len(local_lines):\n",
    "            res_proc_item = line\n",
    "        else:\n",
    "            second_line = local_lines[idx + 1].strip().lower()\n",
    "            if len(second_line) == 0:\n",
    "                del local_lines[idx + 1]\n",
    "                continue\n",
    "            res_proc_item = line + ' ' + second_line\n",
    "        splitted = res_proc_item.split(' ')[:words_limit]\n",
    "        if len(splitted) >= words_at_least:\n",
    "            rap_sentences.append(' '.join(splitted))\n",
    "        idx += lines_in_group\n",
    "    return rap_sentences\n",
    "\n",
    "# TODO how to have a deal with slang?\n",
    "def get_approx_sentences_from_lyrics(lyrics, local_processed_data):\n",
    "    couplets = re.split('\\[.*\\]' ,lyrics)\n",
    "    for couplet in couplets:\n",
    "        local_lines = couplet.split('\\n')\n",
    "        \n",
    "        if len(local_lines) == 1:\n",
    "            continue\n",
    "        lines_in_group = 2\n",
    "        rap_sentences = join_rap_sentences(local_lines, lines_in_group)\n",
    "        local_processed_data.extend(rap_sentences)\n",
    "\n",
    "def get_data_lyrics(substorage, topic_key):\n",
    "    local_processed_data = []\n",
    "    for lyrics in substorage.values():\n",
    "        approx_sentences = get_approx_sentences_from_lyrics(lyrics, local_processed_data)\n",
    "    local_processed_labels = [map_labels[topic_key]] * len(local_processed_data)\n",
    "    with open(f'tmp_{topic_key}.json', 'w') as f:\n",
    "        json.dump({'section' : local_processed_data}, f, indent=4, sort_keys=True)\n",
    "    return local_processed_data, local_processed_labels\n",
    "\n",
    "processed_data = []\n",
    "processed_labels = []\n",
    "for topic_key in map_labels.keys():\n",
    "    if topic_key == 'joke' or topic_key == 'usagov':\n",
    "        local_processed_data, local_processed_labels = get_data_rss(storage_to_process[topic_key], topic_key)\n",
    "        processed_data.extend(local_processed_data)\n",
    "        processed_labels.extend(local_processed_labels)\n",
    "    elif topic_key == 'rap':\n",
    "        local_processed_data, local_processed_labels = get_data_lyrics(storage_to_process[topic_key], topic_key)\n",
    "        processed_data.extend(local_processed_data)\n",
    "        processed_labels.extend(local_processed_labels)\n",
    "    else:\n",
    "        raise Exception(f'Unknown topic_key: {topic_key}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_data = pd.DataFrame({'text' : processed_data, 'label' : processed_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>antony j blinken, secretary of state</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>on behalf of the american people, i would like...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the united states and tanzania enjoy a longsta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>as tanzania also will soon celebrate  years of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in the areas of health, education, governance,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28238</th>\n",
       "      <td>i arrived in front of the dormitory \"yo, could...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28239</th>\n",
       "      <td>they showed me where it was for the moment i d...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28240</th>\n",
       "      <td>so, i came to her room and opened the door oh,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28241</th>\n",
       "      <td>a fella tongue-kissin' my girl in her mouth i ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28242</th>\n",
       "      <td>so please, listen to the message that i send d...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28243 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0                   antony j blinken, secretary of state      0\n",
       "1      on behalf of the american people, i would like...      0\n",
       "2      the united states and tanzania enjoy a longsta...      0\n",
       "3      as tanzania also will soon celebrate  years of...      0\n",
       "4      in the areas of health, education, governance,...      0\n",
       "...                                                  ...    ...\n",
       "28238  i arrived in front of the dormitory \"yo, could...      2\n",
       "28239  they showed me where it was for the moment i d...      2\n",
       "28240  so, i came to her room and opened the door oh,...      2\n",
       "28241  a fella tongue-kissin' my girl in her mouth i ...      2\n",
       "28242  so please, listen to the message that i send d...      2\n",
       "\n",
       "[28243 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    25755\n",
       "2     1351\n",
       "1     1137\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32    4603\n",
       "4     1347\n",
       "8     1341\n",
       "5     1175\n",
       "7     1149\n",
       "9     1137\n",
       "14    1115\n",
       "6     1112\n",
       "11    1069\n",
       "12    1061\n",
       "10    1021\n",
       "17     994\n",
       "13     943\n",
       "16     922\n",
       "15     876\n",
       "18     874\n",
       "21     838\n",
       "19     828\n",
       "22     685\n",
       "23     659\n",
       "20     630\n",
       "24     626\n",
       "26     515\n",
       "28     513\n",
       "25     492\n",
       "27     472\n",
       "30     467\n",
       "29     405\n",
       "31     374\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_data['text'].apply(lambda x: len(x.split(' '))).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce number of samples of the largest class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_of_usagov = df_text_data[df_text_data['label'] == 0].index\n",
    "index_of_usagov\n",
    "remove_n = 24500\n",
    "drop_indices = np.random.choice(index_of_usagov, remove_n, replace=False)\n",
    "df_text_data_red = df_text_data.drop(drop_indices).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1351\n",
       "0    1255\n",
       "1    1137\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_data_red['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace transformer is coming!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = Dataset.from_pandas(df_text_data_red).remove_columns('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 3743\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'validation of an electronic submission via grantsgov can take up to two business days',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'distilgpt2'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65fae6964e1f4c1a9e6cf360f98fa614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3743 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding='max_length', truncation=True, max_length=32, return_tensors='pt')\n",
    "\n",
    "tokenized_datasets = hf_dataset.map(tokenize_function)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"Hello, I'm a language model,\\n\\n\\n\\nNow I'm writing a tutorial to give you a tutorial to add some functionality on the Java\"}, {'generated_text': \"Hello, I'm a language model!\\nThe code below is used for a lot of writing and other stuff.\\nCode below is used for writing\"}, {'generated_text': \"Hello, I'm a language model, so I figured that to give my opinion of this project a try please do not let me know if you have\"}]\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "generated = generator(\"Hello, I'm a language model\", max_length=30, num_return_sequences=3)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"model_storage\")\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d1db2122d3a4f379e30b78522dfab80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 started. Eval on train: 40352.734375. Cum norm: 5464.58984375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e3453834db47de834886251c7cfdef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 started. Eval on train: 40406.15625. Cum norm: 5464.61767578125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20744a13ecfb432399756e9462eeaa63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 started. Eval on train: 40374.3046875. Cum norm: 5464.64794921875\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e8088a938640109c48c87a93bb331a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "trained_model = GPT2LMHeadModel.from_pretrained(\"model_storage\").to(device)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def check_weights_cum_norm(model):\n",
    "    cum_norm = 0\n",
    "    for name, W in model.named_parameters():\n",
    "        cum_norm += W.norm(2)\n",
    "    return cum_norm\n",
    "\n",
    "def eval_on_loader(trained_model, loader):\n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        for batch in train_dataloader:\n",
    "            ids = torch.stack(batch['input_ids'][0]).to(device)\n",
    "            outputs = trained_model(input_ids=ids)\n",
    "            logits = outputs['logits']\n",
    "            sentences = logits.transpose(0, 1)\n",
    "            labels = ids.transpose(0, 1)\n",
    "            \n",
    "            for sentence, label in zip(sentences, labels):\n",
    "                sentence_cloned = sentence.clone()\n",
    "                label_cloned = label.clone()\n",
    "                torch.nn.functional.relu(sentence_cloned, inplace=True)\n",
    "                losses.append(criterion(sentence_cloned, label_cloned))\n",
    "        return sum(losses)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(tokenized_datasets, shuffle=True, batch_size=batch_size)\n",
    "# eval_dataloader = DataLoader(tokenized_datasets, shuffle=True, batch_size=batch_size)\n",
    "optimizer = AdamW(trained_model.parameters(), lr=0.000001)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.85)\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "num_epochs = 3\n",
    "progress_bar_epoch = tqdm(range(num_epochs))\n",
    "\n",
    "num_batches = len(tokenized_datasets) // batch_size\n",
    "if len(tokenized_datasets) % batch_size:\n",
    "    num_batches += 1\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch} started. Eval on train: {eval_on_loader(trained_model, train_dataloader)}. Cum norm: {check_weights_cum_norm(trained_model)}')\n",
    "    progress_bar_batch = tqdm(range(num_batches))\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "#         print(len(batch['input_ids']))\n",
    "        ids = torch.stack(batch['input_ids'][0]).to(device)\n",
    "        outputs = trained_model(input_ids=ids)\n",
    "        logits = outputs['logits']\n",
    "        sentences = logits.transpose(0, 1)\n",
    "        labels = ids.transpose(0, 1)\n",
    "#         print(sentences.shape)\n",
    "#         print(labels.shape)\n",
    "        losses = []\n",
    "        for sentence, label in zip(sentences, labels):\n",
    "            sentence_cloned = sentence.clone()\n",
    "            label_cloned = label.clone()\n",
    "#             print(sentence_cloned.shape, label_cloned.shape)\n",
    "            with torch.no_grad():\n",
    "                torch.nn.functional.relu(sentence_cloned, inplace=True)\n",
    "            losses.append(criterion(sentence_cloned, label_cloned))\n",
    "#             del sentence_cloned, label_cloned\n",
    "#             gc.collect()\n",
    "#             torch.cuda.empty_cache()\n",
    "        \n",
    "        res_loss = sum(losses)\n",
    "        res_loss.backward()\n",
    "        # clip grads?\n",
    "        optimizer.step()\n",
    "        progress_bar_batch.update(1)\n",
    "        del ids, outputs, logits, sentences, labels\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    progress_bar_epoch.update(1)\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'ok boy and girl - what about boys - what about girls - what about girls - what about girls - what about girls - what about girls - what'}, {'generated_text': \"ok boy, there just wasn't enough.\\n“You're just about getting a new friend.“\\n“Yeah, they want\"}, {'generated_text': 'ok boy, it․․․․․․․․․․․․․'}]\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "cpu = torch.device(\"cpu\")\n",
    "trained_model_cpu = trained_model.to(cpu)\n",
    "generator = pipeline('text-generation', model=trained_model, tokenizer=tokenizer)\n",
    "generated = generator(\"ok boy\", max_length=30, num_return_sequences=3)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
