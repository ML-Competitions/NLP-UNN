{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/masdevas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "import scrapy.crawler as crawler\n",
    "from scrapy.utils.log import configure_logging\n",
    "from multiprocessing import Process, Queue\n",
    "from twisted.internet import reactor\n",
    "from pydispatch import dispatcher\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets.arrow_dataset import Dataset\n",
    "# from transformers import AutoTokenizer\n",
    "# from transformers import AutoModelForSequenceClassification\n",
    "# from transformers import DistilGPT2Model\n",
    "# from transformers import DistilGPT2Tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import pipeline\n",
    "from datasets import load_metric\n",
    "\n",
    "import torch\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from lyricsgenius import Genius\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger('nlp')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_web_data(function):\n",
    "    def f(q):\n",
    "        try:\n",
    "            json_data = function()\n",
    "            q.put(json.dumps(json_data))\n",
    "        except Exception as e:\n",
    "            q.put(e)\n",
    "    q = Queue()\n",
    "    p = Process(target=f, args=(q,))\n",
    "    p.start()\n",
    "    result = q.get()\n",
    "    p.join()\n",
    "    gc.collect()\n",
    "    try:\n",
    "        json_data = json.loads(result)\n",
    "    except:\n",
    "        raise result\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USA government RSS channels parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_usagov_rss_data():\n",
    "    storage = {}\n",
    "    try:\n",
    "        class UsaGovSpider(scrapy.Spider):\n",
    "            name = \"usagov_checker\"\n",
    "            start_urls = ['https://www.state.gov/rss-feeds/']\n",
    "            \n",
    "            def parse(self, response, depth=0):\n",
    "                if depth == 0:\n",
    "                    a_selectors = response.xpath(\"//a\")\n",
    "                    for selector in a_selectors:\n",
    "                        link = selector.xpath(\"@href\").extract_first()\n",
    "                        if 'rss' in link:\n",
    "                            yield response.follow(link, self.parse, cb_kwargs={'depth' : 1})\n",
    "                else:\n",
    "                    storage[str(response.url)] = response.body.decode(\"utf-8\")\n",
    "\n",
    "        runner = crawler.CrawlerRunner()\n",
    "        deferred = runner.crawl(UsaGovSpider)\n",
    "        deferred.addBoth(lambda _: reactor.stop())\n",
    "        reactor.run()\n",
    "        return storage\n",
    "    except Exception as e:\n",
    "        return {'exception' : str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joke RSS channels parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_joke_rss_data():\n",
    "    storage = {}\n",
    "    try:\n",
    "        class JokeSpider(scrapy.Spider):\n",
    "            name = \"joke_checker\"\n",
    "            #start_urls = ['https://blog.feedspot.com/jokes_rss_feeds/'] # 403 error - bot detected, access denied\n",
    "            start_urls = [\n",
    "                'http://www.jokesoftheday.net/jokes-feed/', \n",
    "                'http://www.funnyshortjokes.com/feed',\n",
    "                'https://laffgaff.com/feed/',\n",
    "                'https://www.keeplaughingforever.com/blog//blog-feed.xml',\n",
    "                'https://lite92.ca/category/joke-of-the-day/feed/',\n",
    "                'https://www.thelaughline.com/feed/',\n",
    "                'https://www.jokesbykids.com/riddle/feed/',\n",
    "                'https://newbloggycat.com/category/good-clean-jokes/feed/',\n",
    "                'https://www.super-funny.com/feed/',\n",
    "                'http://slay.me/feed',\n",
    "                'https://www.funny-jokes-quotes-sayings.com/funny-jokes.xml',\n",
    "                'http://modest-jokes.blogspot.com/feeds/posts/default?alt=rss',\n",
    "                'https://badkidsjokes.tumblr.com/rss',\n",
    "                'http://chillyjokes.com/chillyjokes/jokes/feed/',\n",
    "                'https://laughbreak.com/pictures/its-okay-to-feed-the-ducks-bread-now/',\n",
    "                'https://acornyjokeaday.tumblr.com/rss',\n",
    "                'https://somejokeshere.blogspot.com/feeds/posts/default?alt=rss'\n",
    "            ]\n",
    "\n",
    "            def parse(self, response, depth=0):\n",
    "                storage[str(response.url)] = response.body.decode(\"utf-8\")\n",
    "                \n",
    "        runner = crawler.CrawlerRunner()\n",
    "        deferred = runner.crawl(JokeSpider)\n",
    "        deferred.addBoth(lambda _: reactor.stop())\n",
    "        reactor.run()\n",
    "        return storage\n",
    "    except Exception as e:\n",
    "        return {'exception' : str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rap texts parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_genius_token():\n",
    "    with open('genius_token.txt', 'r') as f:\n",
    "        token = f.readline().strip()\n",
    "    return token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rap_data():\n",
    "    genius = Genius(read_genius_token())\n",
    "    storage = {}\n",
    "    try:\n",
    "        class RapSpider(scrapy.Spider):\n",
    "            name = \"rap_checker\"\n",
    "            start_urls = [\n",
    "                'https://bestlifeonline.com/funniest-rap-lyrics/'\n",
    "            ]\n",
    "\n",
    "            def parse(self, response, depth=0):\n",
    "                resp = response.xpath(\"//div[@class='content noskimwords']\").xpath(\"//h2/text()\")\n",
    "                for item in resp:\n",
    "                    match = re.match(\"[0-9]+\\. ([A-Za-z0-9\\s\\-\\'\\.]+), \\\"([A-Za-z0-9\\s\\-\\'\\.]+)\\\"\", item.get().strip())\n",
    "                    if match:\n",
    "                        try:\n",
    "                            author = match.group(1)\n",
    "                            song = match.group(2)\n",
    "                            name = author+'|'+song\n",
    "                            print((author, song))\n",
    "                            song = genius.search_song(song, author)\n",
    "    #                         print(song.lyrics)\n",
    "                            print(len(song.lyrics))\n",
    "                            storage[name] = song.lyrics\n",
    "                        except Exception as e:\n",
    "                            print(f'!!! Exception passed on {name}', str(e))\n",
    "                \n",
    "        runner = crawler.CrawlerRunner()\n",
    "        deferred = runner.crawl(RapSpider)\n",
    "        deferred.addBoth(lambda _: reactor.stop())\n",
    "        reactor.run()\n",
    "        return storage\n",
    "    except Exception as e:\n",
    "        return {'exception' : str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING\n"
     ]
    }
   ],
   "source": [
    "storage_file = 'storage.pkl'\n",
    "def is_rawdata_exist(storage_file):\n",
    "    return os.path.exists(storage_file)\n",
    "    # return False\n",
    "\n",
    "if is_rawdata_exist(storage_file):\n",
    "    print('LOADING')\n",
    "    with open(storage_file, 'rb') as f:\n",
    "        storage = pickle.load(f)\n",
    "else:\n",
    "    print('FROM WEB')\n",
    "    storage = {}  \n",
    "    storage['usagov'] = get_web_data(get_usagov_rss_data)\n",
    "    storage['joke'] = get_web_data(get_joke_rss_data)\n",
    "    storage['rap'] = get_web_data(get_rap_data)\n",
    "    with open(storage_file, 'wb') as f:\n",
    "        pickle.dump(storage, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess sentences for HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/masdevas/.anaconda3/envs/nlp-unn/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "storage_to_process = copy.deepcopy(storage)\n",
    "\n",
    "map_labels = {'usagov' : 0, 'joke' : 1, 'rap' : 2}\n",
    "\n",
    "words_limit = 128\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    html_detector = re.compile(r'<.*?>')\n",
    "    return html_detector.sub(r'', text)\n",
    "\n",
    "def remove_links(text):\n",
    "    links_remover = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return links_remover.sub(r'', text)\n",
    "\n",
    "def preprocess_html(body, local_processed_data, topic_key):\n",
    "#     l = 0\n",
    "    soup = BeautifulSoup(body)\n",
    "    p_tags = soup.find_all('p')\n",
    "    for each in p_tags:\n",
    "        processed = sent_tokenize(each.text.encode('ascii', 'ignore').decode('ascii').lower().strip())\n",
    "        for processed_item in processed:\n",
    "            processed_item = re.sub(\"[\\d\\.]+\", '', processed_item).strip()\n",
    "            if len(processed_item) > 0:\n",
    "                if topic_key == 'joke':\n",
    "                    if processed_item.startswith('a: ') or processed_item.startswith('q: '):\n",
    "                        res_proc_item = processed_item[3:]\n",
    "                    else:\n",
    "                        res_proc_item = processed_item\n",
    "                elif topic_key == 'usagov':\n",
    "                    if len(processed_item.split(' ')) > 3:\n",
    "                        res_proc_item = processed_item\n",
    "                else:\n",
    "                    raise Exception(f'Unknown topic_key for preprocess_html(): {topic_key}')\n",
    "                res_proc_item = ' '.join(res_proc_item.split(' ')[:words_limit])\n",
    "                local_processed_data.append(res_proc_item)\n",
    "#             l += len(processed)\n",
    "#     print(l)\n",
    "\n",
    "def get_data_rss(substorage, topic_key):\n",
    "    local_processed_data = []\n",
    "    for link, body in substorage.items():\n",
    "        preprocess_html(body, local_processed_data, topic_key)\n",
    "    local_processed_labels = [map_labels[topic_key]] * len(local_processed_data)\n",
    "    with open(f'tmp_{topic_key}.json', 'w') as f:\n",
    "        json.dump({'section' : local_processed_data}, f, indent=4, sort_keys=True)\n",
    "    return local_processed_data, local_processed_labels\n",
    "\n",
    "def join_rap_sentences(local_lines, lines_in_group):\n",
    "    rap_sentences = []\n",
    "    idx = 0\n",
    "    while idx < len(local_lines):\n",
    "        line = local_lines[idx].strip().lower()\n",
    "        if len(line) == 0:\n",
    "            del local_lines[idx]\n",
    "            continue\n",
    "        if idx + lines_in_group > len(local_lines):\n",
    "            res_proc_item = line\n",
    "        else:\n",
    "            second_line = local_lines[idx + 1].strip().lower()\n",
    "            if len(second_line) == 0:\n",
    "                del local_lines[idx + 1]\n",
    "                continue\n",
    "            res_proc_item = line + ' ' + second_line\n",
    "        rap_sentences.append(' '.join(res_proc_item.split(' ')[:words_limit]))\n",
    "        idx += lines_in_group\n",
    "    return rap_sentences\n",
    "\n",
    "# TODO how to have a deal with slang?\n",
    "def get_approx_sentences_from_lyrics(lyrics, local_processed_data):\n",
    "    couplets = re.split('\\[.*\\]' ,lyrics)\n",
    "    for couplet in couplets:\n",
    "        local_lines = couplet.split('\\n')\n",
    "        \n",
    "        if len(local_lines) == 1:\n",
    "            continue\n",
    "        lines_in_group = 2\n",
    "        rap_sentences = join_rap_sentences(local_lines, lines_in_group)\n",
    "        local_processed_data.extend(rap_sentences)\n",
    "\n",
    "def get_data_lyrics(substorage, topic_key):\n",
    "    local_processed_data = []\n",
    "    for lyrics in substorage.values():\n",
    "        approx_sentences = get_approx_sentences_from_lyrics(lyrics, local_processed_data)\n",
    "    local_processed_labels = [map_labels[topic_key]] * len(local_processed_data)\n",
    "    with open(f'tmp_{topic_key}.json', 'w') as f:\n",
    "        json.dump({'section' : local_processed_data}, f, indent=4, sort_keys=True)\n",
    "    return local_processed_data, local_processed_labels\n",
    "\n",
    "processed_data = []\n",
    "processed_labels = []\n",
    "for topic_key in map_labels.keys():\n",
    "    if topic_key == 'joke' or topic_key == 'usagov':\n",
    "        local_processed_data, local_processed_labels = get_data_rss(storage_to_process[topic_key], topic_key)\n",
    "        processed_data.extend(local_processed_data)\n",
    "        processed_labels.extend(local_processed_labels)\n",
    "    elif topic_key == 'rap':\n",
    "        local_processed_data, local_processed_labels = get_data_lyrics(storage_to_process[topic_key], topic_key)\n",
    "        processed_data.extend(local_processed_data)\n",
    "        processed_labels.extend(local_processed_labels)\n",
    "    else:\n",
    "        raise Exception(f'Unknown topic_key: {topic_key}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_data = pd.DataFrame({'text' : processed_data, 'label' : processed_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>antony j blinken, secretary of state</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>on behalf of the american people, i would like...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the united states and tanzania enjoy a longsta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>as tanzania also will soon celebrate  years of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i send my best wishes to the tanzanian people ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25165</th>\n",
       "      <td>i arrived in front of the dormitory \"yo, could...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25166</th>\n",
       "      <td>they showed me where it was for the moment i d...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25167</th>\n",
       "      <td>so, i came to her room and opened the door oh,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25168</th>\n",
       "      <td>a fella tongue-kissin' my girl in her mouth i ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25169</th>\n",
       "      <td>so please, listen to the message that i send d...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25170 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0                   antony j blinken, secretary of state      0\n",
       "1      on behalf of the american people, i would like...      0\n",
       "2      the united states and tanzania enjoy a longsta...      0\n",
       "3      as tanzania also will soon celebrate  years of...      0\n",
       "4      i send my best wishes to the tanzanian people ...      0\n",
       "...                                                  ...    ...\n",
       "25165  i arrived in front of the dormitory \"yo, could...      2\n",
       "25166  they showed me where it was for the moment i d...      2\n",
       "25167  so, i came to her room and opened the door oh,...      2\n",
       "25168  a fella tongue-kissin' my girl in her mouth i ...      2\n",
       "25169  so please, listen to the message that i send d...      2\n",
       "\n",
       "[25170 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    22494\n",
       "2     1375\n",
       "1     1301\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8      1180\n",
       "4      1061\n",
       "14      985\n",
       "7       961\n",
       "12      945\n",
       "       ... \n",
       "86        2\n",
       "106       2\n",
       "94        2\n",
       "109       2\n",
       "97        2\n",
       "Name: text, Length: 103, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = lambda x: len(x.split(' '))\n",
    "lens = df_text_data['text'].apply(f)\n",
    "lens.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce number of samples of the largest class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_of_usagov = df_text_data[df_text_data['label'] == 0].index\n",
    "index_of_usagov\n",
    "remove_n = 18000\n",
    "drop_indices = np.random.choice(index_of_usagov, remove_n, replace=False)\n",
    "df_text_data_red = df_text_data.drop(drop_indices).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4494\n",
       "2    1375\n",
       "1    1301\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_data_red['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace transformers is coming!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = Dataset.from_pandas(df_text_data_red).remove_columns('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 7170\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"what is will smith's favourite band?\", 'label': 1}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset[5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distilgpt2'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaef71eda1764cdd8aa8080c4d32e773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7170 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding='max_length', truncation=True, max_length=32, return_tensors='pt')\n",
    "\n",
    "tokenized_datasets = hf_dataset.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 7170\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_datasets['input_ids'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antony j blinken, secretary of state\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor(tokenizer.encode(\"hello my name is\", add_special_tokens=False)).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[31373,   616,  1438,   318]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hello, I\\'m a language model, it has a lot of complexity.\\n\\nIt is hard to think of anything beyond the possibility of some kind of \"interpreting\" of a project. What if my job is to help people understand how a project works, the results you see in their reports and the details in the documentation don\\'t always match those of other projects? What if a person is thinking of building on that theory, what if you really want to get into those other projects that are not already in development?\\nIn the future I think that I can get into some aspects of project development and how some other ideas, techniques and code can be implemented more efficiently to prevent issues from happening.\\nAll in all, I know that a lot of people are frustrated in getting into projects (as with many others) because they are interested in getting started on something that is not a great project, or there is always something they are looking forward to when they go back to work. It'},\n",
       " {'generated_text': \"Hello, I'm a language model designer.\\n\\nThis article is about a topic at the end of this post. I am going to make it clear as you know, in this article, in this blog I am going to introduce the various examples of languages that use the most common names and conventions around the universe. For instance, this is a list of three languages, each of which in the main article has a name. Let me try it out.\\nJava is a single-language language:\\nThe second is a single-language language:\\nThe third is a single-language language:\\nThe fourth is a single-language language:\\nThe fifth is a single-language language:\\nThis is a list of three languages, with their names:\\nJava is a single-language language: The sixth is a single-language language, and all other languages from the previous five languages use this code, followed by a list of five languages. For example, if for example,\"},\n",
       " {'generated_text': \"Hello, I'm a language model. This post will try to help you understand the language, the language and how it can help you understand and learn from it. If you could just make this post look nice, please don't hesitate to drop me a line. What is your thoughts on using them, how many times has there been a typo?\\nI don't really care about what I find (it's like I have to read it all the time, so try it out to find out when I miss something. I might be able to talk to you about the last thing anyone needs to remember before I move on to the next topic). Just be sure you don't miss what you are saying - I will do anything to help you.\\nThank you.\\nShare this: Twitter\\nFacebook\\n\\n\\n\\nPinterest\\n\\n\\n\\nReddit\\n\\n\\n\\n\\n\\n\\nGoogle\\n\\n\\n\\n\\n\\nEmail\\n\\n\\nLike this: Like Loading...\"}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Hello, I'm a language model\", max_length=200, num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     do_train=True,\n",
    "#     do_eval=True,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     eval_steps=1000,\n",
    "#     logging_dir=\"exp/bart/logs\",\n",
    "#     per_device_train_batch_size=32,\n",
    "#     per_device_eval_batch_size=32,\n",
    "#     gradient_accumulation_steps=2,\n",
    "#     eval_accumulation_steps=1,\n",
    "#     output_dir=\"test_trainer\",\n",
    "#     num_train_epochs=50)\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=small_train_dataset,\n",
    "#     eval_dataset=small_eval_dataset\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_datasets, shuffle=True, batch_size=8)\n",
    "eval_dataloader = DataLoader(tokenized_datasets, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d7c162b9c142918ed5025cddf8fd08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2691 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[  392,   270,    67,  1050, 43395,   568,    12, 21078],\n",
      "        [ 1312,   318,   824,    76,  2756,   356,  5769,   560],\n",
      "        [  892,   257,   290,   635,    25,  2067,    82, 21019],\n",
      "        [  356, 29210,   262,  7634,   356,  1561,  6513,   268],\n",
      "        [  303,   284,  3230, 18656,   655,   259,  7252,    25],\n",
      "        [  477,   262,  2324,  5745,   466,  3256,   828, 43395],\n",
      "        [ 1775,  1109,  2995,   326,   407,   651,   319,  8900],\n",
      "        [  262,   326,  1448,   423,   423, 43701,  5153,    11],\n",
      "        [13530,   612,   357,  2722,   281,     6, 32316, 12759],\n",
      "        [  351,   389,   786,  4918,  4296,  5385,   739,  2888],\n",
      "        [  543,  2972,    70,   422,   284,  4341,  3670,  6106],\n",
      "        [  674,  9619,     8,   778,  2148,   259, 46955,   354],\n",
      "        [  334,   284,  5597,    76, 50256,     6,   286,    11],\n",
      "        [   74, 10996,   329,   287, 50256,   257,   262,  5875],\n",
      "        [ 3201,  1022,  1933,   262, 50256,  1256,   719,   345],\n",
      "        [  666,   674,   284,  1613, 50256,   286,   329, 50256],\n",
      "        [ 4887,  6905,  1104,   284, 50256,   640,  6829, 50256],\n",
      "        [  423,    11,   262,  1100, 50256,    11,   329, 50256],\n",
      "        [ 9322,   290,  3230,   428, 50256,   523,  4356, 50256],\n",
      "        [  777,   880, 11783,  2665, 50256,   356,  2611, 50256],\n",
      "        [ 9416,  2555,  7756,   355, 50256,   460, 50256, 50256],\n",
      "        [50256,   284,   290,   257, 50256,  1382, 50256, 50256],\n",
      "        [50256,   779,   584, 17434, 50256,   257, 50256, 50256],\n",
      "        [50256,   883,  1688,   372, 50256, 50256, 50256, 50256],\n",
      "        [50256,  9619,  2995, 50256, 50256, 50256, 50256, 50256],\n",
      "        [50256, 50256,   287, 50256, 50256, 50256, 50256, 50256],\n",
      "        [50256, 50256,  1216, 50256, 50256, 50256, 50256, 50256],\n",
      "        [50256, 50256,   590, 50256, 50256, 50256, 50256, 50256],\n",
      "        [50256, 50256,   284, 50256, 50256, 50256, 50256, 50256],\n",
      "        [50256, 50256,  2291, 50256, 50256, 50256, 50256, 50256],\n",
      "        [50256, 50256,   262, 50256, 50256, 50256, 50256, 50256],\n",
      "        [50256, 50256,   294, 50256, 50256, 50256, 50256, 50256]],\n",
      "       device='cuda:0')\n",
      "torch.Size([32, 8])\n",
      "{'input_ids': tensor([[  392,   270,    67,  1050, 43395,   568,    12, 21078],\n",
      "        [ 1312,   318,   824,    76,  2756,   356,  5769,   560],\n",
      "        [  892,   257,   290,   635,    25,  2067,    82, 21019],\n",
      "        [  356, 29210,   262,  7634,   356,  1561,  6513,   268],\n",
      "        [  303,   284,  3230, 18656,   655,   259,  7252,    25],\n",
      "        [  477,   262,  2324,  5745,   466,  3256,   828, 43395],\n",
      "        [ 1775,  1109,  2995,   326,   407,   651,   319,  8900],\n",
      "        [  262,   326,  1448,   423,   423, 43701,  5153,    11],\n",
      "        [13530,   612,   357,  2722,   281,     6, 32316, 12759],\n",
      "        [  351,   389,   786,  4918,  4296,  5385,   739,  2888],\n",
      "        [  543,  2972,    70,   422,   284,  4341,  3670,  6106],\n",
      "        [  674,  9619,     8,   778,  2148,   259, 46955,   354],\n",
      "        [  334,   284,  5597,    76, 50256,     6,   286,    11],\n",
      "        [   74, 10996,   329,   287, 50256,   257,   262,  5875],\n",
      "        [ 3201,  1022,  1933,   262, 50256,  1256,   719,   345],\n",
      "        [  666,   674,   284,  1613, 50256,   286,   329, 50256],\n",
      "        [ 4887,  6905,  1104,   284, 50256,   640,  6829, 50256],\n",
      "        [  423,    11,   262,  1100, 50256,    11,   329, 50256],\n",
      "        [ 9322,   290,  3230,   428, 50256,   523,  4356, 50256],\n",
      "        [  777,   880, 11783,  2665, 50256,   356,  2611, 50256],\n",
      "        [ 9416,  2555,  7756,   355, 50256,   460, 50256, 50256],\n",
      "        [50256,   284,   290,   257, 50256,  1382, 50256, 50256],\n",
      "        [50256,   779,   584, 17434, 50256,   257, 50256, 50256],\n",
      "        [50256,   883,  1688,   372, 50256, 50256, 50256, 50256],\n",
      "        [50256,  9619,  2995, 50256, 50256, 50256, 50256, 50256],\n",
      "        [50256, 50256,   287, 50256, 50256, 50256, 50256, 50256],\n",
      "        [50256, 50256,  1216, 50256, 50256, 50256, 50256, 50256],\n",
      "        [50256, 50256,   590, 50256, 50256, 50256, 50256, 50256],\n",
      "        [50256, 50256,   284, 50256, 50256, 50256, 50256, 50256],\n",
      "        [50256, 50256,  2291, 50256, 50256, 50256, 50256, 50256],\n",
      "        [50256, 50256,   262, 50256, 50256, 50256, 50256, 50256],\n",
      "        [50256, 50256,   294, 50256, 50256, 50256, 50256, 50256]],\n",
      "       device='cuda:0'), 'past_key_values': None, 'use_cache': None, 'position_ids': None, 'attention_mask': None, 'token_type_ids': None}\n",
      "torch.Size([32, 8])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sarfw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_52969/2759754829.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0msarfw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;31m#         batch = {k: v.to(device) for k, v in batch.items()}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m#         outputs = model(**batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sarfw' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        print(len(batch['input_ids']))\n",
    "#         print(batch)\n",
    "#         print(batch.keys())\n",
    "#         print(len(batch['text']))\n",
    "        ids = torch.stack(batch['input_ids'][0]).to(device)\n",
    "#         attention_mask = torch.stack(batch['attention_mask'][0]).to(device)\n",
    "        print(ids)\n",
    "        print(ids.shape)\n",
    "#         print(attention_mask.shape)\n",
    "#         outputs = model(input_ids=ids, attention_mask=attention_mask)\n",
    "#         print(outputs)\n",
    "#         print(outputs.logits.shape)\n",
    "#         print(outputs)\n",
    "#         print(outputs.logits)\n",
    "#         print(outputs.logits.shape)\n",
    "#         print('#' * 50)\n",
    "\n",
    "        inputs_for_gener = model.prepare_inputs_for_generation(input_ids=ids)\n",
    "\n",
    "    #         print(inputs_for_gener['perm_mask'].shape)\n",
    "#         print(inputs_for_gener['target_mapping'].shape)\n",
    "        print(inputs_for_gener)\n",
    "        print(inputs_for_gener['input_ids'].shape)\n",
    "    \n",
    "#         outputs = model(input_ids=inputs_for_gener['input_ids'], perm_mask=inputs_for_gener['perm_mask'], \n",
    "#                         target_mapping=inputs_for_gener['target_mapping'])\n",
    "#         print(outputs)\n",
    "#         print(outputs.logits.shape)\n",
    "        \n",
    "    \n",
    "        sarfw\n",
    "#         batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#         outputs = model(**batch)\n",
    "#         loss = outputs.loss\n",
    "#         loss.backward()\n",
    "\n",
    "#         optimizer.step()\n",
    "#         lr_scheduler.step()\n",
    "#         optimizer.zero_grad()\n",
    "#         progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
