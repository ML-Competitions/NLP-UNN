{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/masdevas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "import scrapy.crawler as crawler\n",
    "from scrapy.utils.log import configure_logging\n",
    "from multiprocessing import Process, Queue\n",
    "from twisted.internet import reactor\n",
    "from pydispatch import dispatcher\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets.arrow_dataset import Dataset\n",
    "# from transformers import AutoTokenizer\n",
    "# from transformers import AutoModelForSequenceClassification\n",
    "# from transformers import DistilGPT2Model\n",
    "# from transformers import DistilGPT2Tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import pipeline\n",
    "\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from lyricsgenius import Genius\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger('nlp')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_web_data(function):\n",
    "    def f(q):\n",
    "        try:\n",
    "            json_data = function()\n",
    "            q.put(json.dumps(json_data))\n",
    "        except Exception as e:\n",
    "            q.put(e)\n",
    "    q = Queue()\n",
    "    p = Process(target=f, args=(q,))\n",
    "    p.start()\n",
    "    result = q.get()\n",
    "    p.join()\n",
    "    gc.collect()\n",
    "    try:\n",
    "        json_data = json.loads(result)\n",
    "    except:\n",
    "        raise result\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USA government RSS channels parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_usagov_rss_data():\n",
    "    storage = {}\n",
    "    try:\n",
    "        class UsaGovSpider(scrapy.Spider):\n",
    "            name = \"usagov_checker\"\n",
    "            start_urls = ['https://www.state.gov/rss-feeds/']\n",
    "            \n",
    "            def parse(self, response, depth=0):\n",
    "                if depth == 0:\n",
    "                    a_selectors = response.xpath(\"//a\")\n",
    "                    for selector in a_selectors:\n",
    "                        link = selector.xpath(\"@href\").extract_first()\n",
    "                        if 'rss' in link:\n",
    "                            yield response.follow(link, self.parse, cb_kwargs={'depth' : 1})\n",
    "                else:\n",
    "                    storage[str(response.url)] = response.body.decode(\"utf-8\")\n",
    "\n",
    "        runner = crawler.CrawlerRunner()\n",
    "        deferred = runner.crawl(UsaGovSpider)\n",
    "        deferred.addBoth(lambda _: reactor.stop())\n",
    "        reactor.run()\n",
    "        return storage\n",
    "    except Exception as e:\n",
    "        return {'exception' : str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joke RSS channels parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_joke_rss_data():\n",
    "    storage = {}\n",
    "    try:\n",
    "        class JokeSpider(scrapy.Spider):\n",
    "            name = \"joke_checker\"\n",
    "            #start_urls = ['https://blog.feedspot.com/jokes_rss_feeds/'] # 403 error - bot detected, access denied\n",
    "            start_urls = [\n",
    "                'http://www.jokesoftheday.net/jokes-feed/', \n",
    "                'http://www.funnyshortjokes.com/feed',\n",
    "                'https://laffgaff.com/feed/',\n",
    "                'https://www.keeplaughingforever.com/blog//blog-feed.xml',\n",
    "                'https://lite92.ca/category/joke-of-the-day/feed/',\n",
    "                'https://www.thelaughline.com/feed/',\n",
    "                'https://www.jokesbykids.com/riddle/feed/',\n",
    "                'https://newbloggycat.com/category/good-clean-jokes/feed/',\n",
    "                'https://www.super-funny.com/feed/',\n",
    "                'http://slay.me/feed',\n",
    "                'https://www.funny-jokes-quotes-sayings.com/funny-jokes.xml',\n",
    "                'http://modest-jokes.blogspot.com/feeds/posts/default?alt=rss',\n",
    "                'https://badkidsjokes.tumblr.com/rss',\n",
    "                'http://chillyjokes.com/chillyjokes/jokes/feed/',\n",
    "                'https://laughbreak.com/pictures/its-okay-to-feed-the-ducks-bread-now/',\n",
    "                'https://acornyjokeaday.tumblr.com/rss',\n",
    "                'https://somejokeshere.blogspot.com/feeds/posts/default?alt=rss'\n",
    "            ]\n",
    "\n",
    "            def parse(self, response, depth=0):\n",
    "                storage[str(response.url)] = response.body.decode(\"utf-8\")\n",
    "                \n",
    "        runner = crawler.CrawlerRunner()\n",
    "        deferred = runner.crawl(JokeSpider)\n",
    "        deferred.addBoth(lambda _: reactor.stop())\n",
    "        reactor.run()\n",
    "        return storage\n",
    "    except Exception as e:\n",
    "        return {'exception' : str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rap texts parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_genius_token():\n",
    "    with open('genius_token.txt', 'r') as f:\n",
    "        token = f.readline().strip()\n",
    "    return token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rap_data():\n",
    "    genius = Genius(read_genius_token())\n",
    "    storage = {}\n",
    "    try:\n",
    "        class RapSpider(scrapy.Spider):\n",
    "            name = \"rap_checker\"\n",
    "            start_urls = [\n",
    "                'https://bestlifeonline.com/funniest-rap-lyrics/'\n",
    "            ]\n",
    "\n",
    "            def parse(self, response, depth=0):\n",
    "                resp = response.xpath(\"//div[@class='content noskimwords']\").xpath(\"//h2/text()\")\n",
    "                for item in resp:\n",
    "                    match = re.match(\"[0-9]+\\. ([A-Za-z0-9\\s\\-\\'\\.]+), \\\"([A-Za-z0-9\\s\\-\\'\\.]+)\\\"\", item.get().strip())\n",
    "                    if match:\n",
    "                        try:\n",
    "                            author = match.group(1)\n",
    "                            song = match.group(2)\n",
    "                            name = author+'|'+song\n",
    "                            print((author, song))\n",
    "                            song = genius.search_song(song, author)\n",
    "    #                         print(song.lyrics)\n",
    "                            print(len(song.lyrics))\n",
    "                            storage[name] = song.lyrics\n",
    "                        except Exception as e:\n",
    "                            print(f'!!! Exception passed on {name}', str(e))\n",
    "                \n",
    "        runner = crawler.CrawlerRunner()\n",
    "        deferred = runner.crawl(RapSpider)\n",
    "        deferred.addBoth(lambda _: reactor.stop())\n",
    "        reactor.run()\n",
    "        return storage\n",
    "    except Exception as e:\n",
    "        return {'exception' : str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING\n"
     ]
    }
   ],
   "source": [
    "storage_file = 'storage.pkl'\n",
    "def is_rawdata_exist(storage_file):\n",
    "    return os.path.exists(storage_file)\n",
    "    # return False\n",
    "\n",
    "if is_rawdata_exist(storage_file):\n",
    "    print('LOADING')\n",
    "    with open(storage_file, 'rb') as f:\n",
    "        storage = pickle.load(f)\n",
    "else:\n",
    "    print('FROM WEB')\n",
    "    storage = {}  \n",
    "    storage['usagov'] = get_web_data(get_usagov_rss_data)\n",
    "    storage['joke'] = get_web_data(get_joke_rss_data)\n",
    "    storage['rap'] = get_web_data(get_rap_data)\n",
    "    with open(storage_file, 'wb') as f:\n",
    "        pickle.dump(storage, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess sentences for HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/masdevas/.anaconda3/envs/nlp-unn/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "storage_to_process = copy.deepcopy(storage)\n",
    "\n",
    "map_labels = {'usagov' : 0, 'joke' : 1, 'rap' : 2}\n",
    "\n",
    "words_limit = 32\n",
    "words_at_least = 4\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    html_detector = re.compile(r'<.*?>')\n",
    "    return html_detector.sub(r'', text)\n",
    "\n",
    "def remove_links(text):\n",
    "    links_remover = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return links_remover.sub(r'', text)\n",
    "\n",
    "def split_sentence_to_approproate_length(res_proc_item):\n",
    "    sentences = []\n",
    "    splitted = res_proc_item.split(' ')\n",
    "    for idx in range(0, len(splitted), words_limit):\n",
    "        part_of_words = splitted[idx:idx+words_limit]\n",
    "        if len(part_of_words) < words_at_least:\n",
    "            continue\n",
    "        else:\n",
    "            sentences.append(' '.join(part_of_words))\n",
    "    return sentences\n",
    "\n",
    "def preprocess_html(body, local_processed_data, topic_key):\n",
    "#     l = 0\n",
    "    soup = BeautifulSoup(body)\n",
    "    p_tags = soup.find_all('p')\n",
    "    for each in p_tags:\n",
    "        processed = sent_tokenize(each.text.encode('ascii', 'ignore').decode('ascii').lower().strip())\n",
    "        for processed_item in processed:\n",
    "            processed_item = re.sub(\"[\\d\\.]+\", '', processed_item).strip()\n",
    "            if len(processed_item) > 0:\n",
    "                if topic_key == 'joke':\n",
    "                    if processed_item.startswith('a: ') or processed_item.startswith('q: '):\n",
    "                        res_proc_item = processed_item[3:]\n",
    "                    else:\n",
    "                        res_proc_item = processed_item\n",
    "                elif topic_key == 'usagov':\n",
    "                    if len(processed_item.split(' ')) > 3:\n",
    "                        res_proc_item = processed_item\n",
    "                else:\n",
    "                    raise Exception(f'Unknown topic_key for preprocess_html(): {topic_key}')\n",
    "#                 res_proc_item = ' '.join([:words_limit])\n",
    "                local_processed_data.extend(split_sentence_to_approproate_length(res_proc_item))\n",
    "#             l += len(processed)\n",
    "#     print(l)\n",
    "\n",
    "def get_data_rss(substorage, topic_key):\n",
    "    local_processed_data = []\n",
    "    for link, body in substorage.items():\n",
    "        preprocess_html(body, local_processed_data, topic_key)\n",
    "    local_processed_labels = [map_labels[topic_key]] * len(local_processed_data)\n",
    "    with open(f'tmp_{topic_key}.json', 'w') as f:\n",
    "        json.dump({'section' : local_processed_data}, f, indent=4, sort_keys=True)\n",
    "    return local_processed_data, local_processed_labels\n",
    "\n",
    "def join_rap_sentences(local_lines, lines_in_group):\n",
    "    rap_sentences = []\n",
    "    idx = 0\n",
    "    while idx < len(local_lines):\n",
    "        line = local_lines[idx].strip().lower()\n",
    "        if len(line) == 0:\n",
    "            del local_lines[idx]\n",
    "            continue\n",
    "        if idx + lines_in_group > len(local_lines):\n",
    "            res_proc_item = line\n",
    "        else:\n",
    "            second_line = local_lines[idx + 1].strip().lower()\n",
    "            if len(second_line) == 0:\n",
    "                del local_lines[idx + 1]\n",
    "                continue\n",
    "            res_proc_item = line + ' ' + second_line\n",
    "        splitted = res_proc_item.split(' ')[:words_limit]\n",
    "        if len(splitted) >= words_at_least:\n",
    "            rap_sentences.append(' '.join(splitted))\n",
    "        idx += lines_in_group\n",
    "    return rap_sentences\n",
    "\n",
    "# TODO how to have a deal with slang?\n",
    "def get_approx_sentences_from_lyrics(lyrics, local_processed_data):\n",
    "    couplets = re.split('\\[.*\\]' ,lyrics)\n",
    "    for couplet in couplets:\n",
    "        local_lines = couplet.split('\\n')\n",
    "        \n",
    "        if len(local_lines) == 1:\n",
    "            continue\n",
    "        lines_in_group = 2\n",
    "        rap_sentences = join_rap_sentences(local_lines, lines_in_group)\n",
    "        local_processed_data.extend(rap_sentences)\n",
    "\n",
    "def get_data_lyrics(substorage, topic_key):\n",
    "    local_processed_data = []\n",
    "    for lyrics in substorage.values():\n",
    "        approx_sentences = get_approx_sentences_from_lyrics(lyrics, local_processed_data)\n",
    "    local_processed_labels = [map_labels[topic_key]] * len(local_processed_data)\n",
    "    with open(f'tmp_{topic_key}.json', 'w') as f:\n",
    "        json.dump({'section' : local_processed_data}, f, indent=4, sort_keys=True)\n",
    "    return local_processed_data, local_processed_labels\n",
    "\n",
    "processed_data = []\n",
    "processed_labels = []\n",
    "for topic_key in map_labels.keys():\n",
    "    if topic_key == 'joke' or topic_key == 'usagov':\n",
    "        local_processed_data, local_processed_labels = get_data_rss(storage_to_process[topic_key], topic_key)\n",
    "        processed_data.extend(local_processed_data)\n",
    "        processed_labels.extend(local_processed_labels)\n",
    "    elif topic_key == 'rap':\n",
    "        local_processed_data, local_processed_labels = get_data_lyrics(storage_to_process[topic_key], topic_key)\n",
    "        processed_data.extend(local_processed_data)\n",
    "        processed_labels.extend(local_processed_labels)\n",
    "    else:\n",
    "        raise Exception(f'Unknown topic_key: {topic_key}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_data = pd.DataFrame({'text' : processed_data, 'label' : processed_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>antony j blinken, secretary of state</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>on behalf of the american people, i would like...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the united states and tanzania enjoy a longsta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>as tanzania also will soon celebrate  years of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in the areas of health, education, governance,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28238</th>\n",
       "      <td>i arrived in front of the dormitory \"yo, could...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28239</th>\n",
       "      <td>they showed me where it was for the moment i d...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28240</th>\n",
       "      <td>so, i came to her room and opened the door oh,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28241</th>\n",
       "      <td>a fella tongue-kissin' my girl in her mouth i ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28242</th>\n",
       "      <td>so please, listen to the message that i send d...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28243 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0                   antony j blinken, secretary of state      0\n",
       "1      on behalf of the american people, i would like...      0\n",
       "2      the united states and tanzania enjoy a longsta...      0\n",
       "3      as tanzania also will soon celebrate  years of...      0\n",
       "4      in the areas of health, education, governance,...      0\n",
       "...                                                  ...    ...\n",
       "28238  i arrived in front of the dormitory \"yo, could...      2\n",
       "28239  they showed me where it was for the moment i d...      2\n",
       "28240  so, i came to her room and opened the door oh,...      2\n",
       "28241  a fella tongue-kissin' my girl in her mouth i ...      2\n",
       "28242  so please, listen to the message that i send d...      2\n",
       "\n",
       "[28243 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    25755\n",
       "2     1351\n",
       "1     1137\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32    4603\n",
       "4     1347\n",
       "8     1341\n",
       "5     1175\n",
       "7     1149\n",
       "9     1137\n",
       "14    1115\n",
       "6     1112\n",
       "11    1069\n",
       "12    1061\n",
       "10    1021\n",
       "17     994\n",
       "13     943\n",
       "16     922\n",
       "15     876\n",
       "18     874\n",
       "21     838\n",
       "19     828\n",
       "22     685\n",
       "23     659\n",
       "20     630\n",
       "24     626\n",
       "26     515\n",
       "28     513\n",
       "25     492\n",
       "27     472\n",
       "30     467\n",
       "29     405\n",
       "31     374\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_data['text'].apply(lambda x: len(x.split(' '))).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce number of samples of the largest class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_of_usagov = df_text_data[df_text_data['label'] == 0].index\n",
    "index_of_usagov\n",
    "remove_n = 24500\n",
    "drop_indices = np.random.choice(index_of_usagov, remove_n, replace=False)\n",
    "df_text_data_red = df_text_data.drop(drop_indices).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1351\n",
       "0    1255\n",
       "1    1137\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_data_red['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace transformer is coming!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = Dataset.from_pandas(df_text_data_red).remove_columns('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 3743\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'additionally, there have been credible reports of targeting of investigative journalists and restrictions on media content',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'distilgpt2'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a756f482581040acb6319051bcfca653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3743 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding='max_length', truncation=True, max_length=32, return_tensors='pt')\n",
    "\n",
    "tokenized_datasets = hf_dataset.map(tokenize_function)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 3743\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"Hello, I'm a language model by which I create and use programming language functions to program in the language and in the languages, such as Java.\"}, {'generated_text': \"Hello, I'm a language model.\\nThis is a language that requires a framework to handle, and that's a very good point. I've\"}, {'generated_text': \"Hello, I'm a language model and I know a bit about it. I don't get the impression people want to hear of you, I'm\"}]\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "generated = generator(\"Hello, I'm a language model\", max_length=30, num_return_sequences=3)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "933ad236596c4deca37a8d0df7c2bdea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 started. Eval on train: 689.0586547851562. Cum norm: 5464.58984375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f2cf5cae6649b49b75ebca96696713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 started. Eval on train: 672.4696044921875. Cum norm: 5586.49755859375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ef245905b8445b9319b82ece1b508e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 started. Eval on train: 322.13385009765625. Cum norm: 5747.8359375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15dd2647349d4692b6e9a589c6da2cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 started. Eval on train: 154.06900024414062. Cum norm: 5842.2724609375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "914a179d2b254e4b85e5bc037b7c5851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_71185/2040985119.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mres_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# clip grads?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mprogress_bar_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/nlp-unn/lib/python3.9/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/nlp-unn/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/nlp-unn/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/nlp-unn/lib/python3.9/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             F.adamw(params_with_grad,\n\u001b[0m\u001b[1;32m    146\u001b[0m                     \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                     \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/nlp-unn/lib/python3.9/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"model_storage\")\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2LMHeadModel\n",
    "# from transformers import GPT2Config\n",
    "\n",
    "# cpu = torch.device(\"cpu\")\n",
    "# model_state_dict = model.state_dict()\n",
    "device = torch.device(\"cuda\")\n",
    "trained_model = GPT2LMHeadModel.from_pretrained(\"model_storage\").to(device)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def check_weights_cum_norm(model):\n",
    "    cum_norm = 0\n",
    "    for name, W in model.named_parameters():\n",
    "        cum_norm += W.norm(2)\n",
    "    return cum_norm\n",
    "\n",
    "def eval_on_loader(trained_model, loader):\n",
    "    with torch.no_grad():\n",
    "    for batch in train_dataloader:\n",
    "        ids = torch.stack(batch['input_ids'][0]).to(device)\n",
    "        outputs = trained_model(input_ids=ids)\n",
    "        logits = outputs['logits']\n",
    "        sentences = logits.transpose(0, 1)\n",
    "        labels = ids.transpose(0, 1)\n",
    "        losses = []\n",
    "        for sentence, label in zip(sentences, labels):\n",
    "            sentence_cloned = sentence.clone()\n",
    "            label_cloned = label.clone()\n",
    "            with torch.no_grad():\n",
    "                torch.nn.functional.relu(sentence_cloned, inplace=True)\n",
    "            losses.append(criterion(sentence_cloned, label_cloned))\n",
    "        return sum(losses)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(tokenized_datasets, shuffle=True, batch_size=batch_size)\n",
    "# eval_dataloader = DataLoader(tokenized_datasets, shuffle=True, batch_size=batch_size)\n",
    "optimizer = AdamW(trained_model.parameters(), lr=0.001)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.85)\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "num_epochs = 10\n",
    "progress_bar_epoch = tqdm(range(num_epochs))\n",
    "\n",
    "num_batches = len(tokenized_datasets) // batch_size\n",
    "if len(tokenized_datasets) % batch_size:\n",
    "    num_batches += 1\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch} started. Eval on train: {eval_on_loader(trained_model, train_dataloader)}. Cum norm: {check_weights_cum_norm(trained_model)}')\n",
    "    progress_bar_batch = tqdm(range(num_batches))\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "#         print(len(batch['input_ids']))\n",
    "        ids = torch.stack(batch['input_ids'][0]).to(device)\n",
    "        outputs = trained_model(input_ids=ids)\n",
    "        logits = outputs['logits']\n",
    "        sentences = logits.transpose(0, 1)\n",
    "        labels = ids.transpose(0, 1)\n",
    "#         print(sentences.shape)\n",
    "#         print(labels.shape)\n",
    "        losses = []\n",
    "        for sentence, label in zip(sentences, labels):\n",
    "            sentence_cloned = sentence.clone()\n",
    "            label_cloned = label.clone()\n",
    "#             print(sentence_cloned.shape, label_cloned.shape)\n",
    "            with torch.no_grad():\n",
    "                torch.nn.functional.relu(sentence_cloned, inplace=True)\n",
    "            losses.append(criterion(sentence_cloned, label_cloned))\n",
    "        \n",
    "        res_loss = sum(losses)\n",
    "        res_loss.backward()\n",
    "        # clip grads?\n",
    "        optimizer.step()\n",
    "        progress_bar_batch.update(1)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    progress_bar_epoch.update(1)\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_cpu = trained_model.to(cpu)\n",
    "generator = pipeline('text-generation', model=trained_model, tokenizer=tokenizer)\n",
    "generated = generator(\"hello\", max_length=30, num_return_sequences=3)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
