{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/masdevas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "import scrapy.crawler as crawler\n",
    "from scrapy.utils.log import configure_logging\n",
    "from multiprocessing import Process, Queue\n",
    "from twisted.internet import reactor\n",
    "from pydispatch import dispatcher\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets.arrow_dataset import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_metric\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from lyricsgenius import Genius\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger('nlp')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset yelp_review_full (/home/masdevas/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50599af8283044b2b68922eca0f464df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"yelp_review_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'text'],\n",
       "    num_rows: 650000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\"a\": [1, 2, 3]})\n",
    "df1_ = Dataset.from_pandas(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['a'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_web_data(function):\n",
    "    def f(q):\n",
    "        try:\n",
    "            json_data = function()\n",
    "            q.put(json.dumps(json_data))\n",
    "        except Exception as e:\n",
    "            q.put(e)\n",
    "    q = Queue()\n",
    "    p = Process(target=f, args=(q,))\n",
    "    p.start()\n",
    "    result = q.get()\n",
    "    p.join()\n",
    "    gc.collect()\n",
    "    try:\n",
    "        json_data = json.loads(result)\n",
    "    except:\n",
    "        raise result\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USA government RSS channels parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_usagov_rss_data():\n",
    "    storage = {}\n",
    "    try:\n",
    "        class UsaGovSpider(scrapy.Spider):\n",
    "            name = \"usagov_checker\"\n",
    "            start_urls = ['https://www.state.gov/rss-feeds/']\n",
    "            \n",
    "            def parse(self, response, depth=0):\n",
    "                if depth == 0:\n",
    "                    a_selectors = response.xpath(\"//a\")\n",
    "                    for selector in a_selectors:\n",
    "                        link = selector.xpath(\"@href\").extract_first()\n",
    "                        if 'rss' in link:\n",
    "                            yield response.follow(link, self.parse, cb_kwargs={'depth' : 1})\n",
    "                else:\n",
    "                    storage[str(response.url)] = response.body.decode(\"utf-8\")\n",
    "\n",
    "        runner = crawler.CrawlerRunner()\n",
    "        deferred = runner.crawl(UsaGovSpider)\n",
    "        deferred.addBoth(lambda _: reactor.stop())\n",
    "        reactor.run()\n",
    "        return storage\n",
    "    except Exception as e:\n",
    "        return {'exception' : str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joke RSS channels parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_joke_rss_data():\n",
    "    storage = {}\n",
    "    try:\n",
    "        class JokeSpider(scrapy.Spider):\n",
    "            name = \"joke_checker\"\n",
    "            #start_urls = ['https://blog.feedspot.com/jokes_rss_feeds/'] # 403 error - bot detected, access denied\n",
    "            start_urls = [\n",
    "                'http://www.jokesoftheday.net/jokes-feed/', \n",
    "                'http://www.funnyshortjokes.com/feed',\n",
    "                'https://laffgaff.com/feed/',\n",
    "                'https://www.keeplaughingforever.com/blog//blog-feed.xml',\n",
    "                'https://lite92.ca/category/joke-of-the-day/feed/',\n",
    "                'https://www.thelaughline.com/feed/',\n",
    "                'https://www.jokesbykids.com/riddle/feed/',\n",
    "                'https://newbloggycat.com/category/good-clean-jokes/feed/',\n",
    "                'https://www.super-funny.com/feed/',\n",
    "                'http://slay.me/feed',\n",
    "                'https://www.funny-jokes-quotes-sayings.com/funny-jokes.xml',\n",
    "                'http://modest-jokes.blogspot.com/feeds/posts/default?alt=rss',\n",
    "                'https://badkidsjokes.tumblr.com/rss',\n",
    "                'http://chillyjokes.com/chillyjokes/jokes/feed/',\n",
    "                'https://laughbreak.com/pictures/its-okay-to-feed-the-ducks-bread-now/',\n",
    "                'https://acornyjokeaday.tumblr.com/rss',\n",
    "                'https://somejokeshere.blogspot.com/feeds/posts/default?alt=rss'\n",
    "            ]\n",
    "\n",
    "            def parse(self, response, depth=0):\n",
    "                storage[str(response.url)] = response.body.decode(\"utf-8\")\n",
    "                \n",
    "        runner = crawler.CrawlerRunner()\n",
    "        deferred = runner.crawl(JokeSpider)\n",
    "        deferred.addBoth(lambda _: reactor.stop())\n",
    "        reactor.run()\n",
    "        return storage\n",
    "    except Exception as e:\n",
    "        return {'exception' : str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rap texts parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_genius_token():\n",
    "    with open('genius_token.txt', 'r') as f:\n",
    "        token = f.readline().strip()\n",
    "    return token\n",
    "\n",
    "genius = Genius(read_genius_token())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rap_data():\n",
    "    storage = {}\n",
    "    try:\n",
    "        class RapSpider(scrapy.Spider):\n",
    "            name = \"rap_checker\"\n",
    "            start_urls = [\n",
    "                'https://bestlifeonline.com/funniest-rap-lyrics/'\n",
    "            ]\n",
    "\n",
    "            def parse(self, response, depth=0):\n",
    "                resp = response.xpath(\"//div[@class='content noskimwords']\").xpath(\"//h2/text()\")\n",
    "                for item in resp:\n",
    "                    match = re.match(\"[0-9]+\\. ([A-Za-z0-9\\s\\-\\'\\.]+), \\\"([A-Za-z0-9\\s\\-\\'\\.]+)\\\"\", item.get().strip())\n",
    "                    if match:\n",
    "                        try:\n",
    "                            author = match.group(1)\n",
    "                            song = match.group(2)\n",
    "                            name = author+'|'+song\n",
    "                            print((author, song))\n",
    "                            song = genius.search_song(song, author)\n",
    "    #                         print(song.lyrics)\n",
    "                            print(len(song.lyrics))\n",
    "                            storage[name] = song.lyrics\n",
    "                        except Exception as e:\n",
    "                            print(f'!!! Exception passed on {name}', str(e))\n",
    "                \n",
    "        runner = crawler.CrawlerRunner()\n",
    "        deferred = runner.crawl(RapSpider)\n",
    "        deferred.addBoth(lambda _: reactor.stop())\n",
    "        reactor.run()\n",
    "        return storage\n",
    "    except Exception as e:\n",
    "        return {'exception' : str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING\n"
     ]
    }
   ],
   "source": [
    "storage_file = 'storage.pkl'\n",
    "def is_rawdata_exist(storage_file):\n",
    "    return os.path.exists(storage_file)\n",
    "    # return False\n",
    "\n",
    "if is_rawdata_exist(storage_file):\n",
    "    print('LOADING')\n",
    "    with open(storage_file, 'rb') as f:\n",
    "        storage = pickle.load(f)\n",
    "else:\n",
    "    print('FROM WEB')\n",
    "    storage = {}  \n",
    "    storage['usagov'] = get_web_data(get_usagov_rss_data)\n",
    "    storage['joke'] = get_web_data(get_joke_rss_data)\n",
    "    storage['rap'] = get_web_data(get_rap_data)\n",
    "    with open(storage_file, 'wb') as f:\n",
    "        pickle.dump(storage, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess sentences for HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/masdevas/.anaconda3/envs/nlp-unn/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "storage_to_process = copy.deepcopy(storage)\n",
    "\n",
    "map_labels = {'usagov' : 0, 'joke' : 1, 'rap' : 2}\n",
    "\n",
    "words_limit = 120\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    html_detector = re.compile(r'<.*?>')\n",
    "    return html_detector.sub(r'', text)\n",
    "\n",
    "def remove_links(text):\n",
    "    links_remover = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return links_remover.sub(r'', text)\n",
    "\n",
    "def preprocess_html(body, local_processed_data, topic_key):\n",
    "#     l = 0\n",
    "    soup = BeautifulSoup(body)\n",
    "    p_tags = soup.find_all('p')\n",
    "    for each in p_tags:\n",
    "        processed = sent_tokenize(each.text.encode('ascii', 'ignore').decode('ascii').lower().strip())\n",
    "        for processed_item in processed:\n",
    "            processed_item = re.sub(\"[\\d\\.]+\", '', processed_item).strip()\n",
    "            if len(processed_item) > 0:\n",
    "                if topic_key == 'joke':\n",
    "                    if processed_item.startswith('a: ') or processed_item.startswith('q: '):\n",
    "                        res_proc_item = processed_item[3:]\n",
    "                    else:\n",
    "                        res_proc_item = processed_item\n",
    "                elif topic_key == 'usagov':\n",
    "                    if len(processed_item.split(' ')) > 3:\n",
    "                        res_proc_item = processed_item\n",
    "                else:\n",
    "                    raise Exception(f'Unknown topic_key for preprocess_html(): {topic_key}')\n",
    "                res_proc_item = ' '.join(res_proc_item.split(' ')[:words_limit])\n",
    "                local_processed_data.append(res_proc_item)\n",
    "#             l += len(processed)\n",
    "#     print(l)\n",
    "\n",
    "def get_data_rss(substorage, topic_key):\n",
    "    local_processed_data = []\n",
    "    for link, body in substorage.items():\n",
    "        preprocess_html(body, local_processed_data, topic_key)\n",
    "    local_processed_labels = [map_labels[topic_key]] * len(local_processed_data)\n",
    "    with open(f'tmp_{topic_key}.json', 'w') as f:\n",
    "        json.dump({'section' : local_processed_data}, f, indent=4, sort_keys=True)\n",
    "    return local_processed_data, local_processed_labels\n",
    "\n",
    "def join_rap_sentences(local_lines, lines_in_group):\n",
    "    rap_sentences = []\n",
    "    idx = 0\n",
    "    while idx < len(local_lines):\n",
    "        line = local_lines[idx].strip().lower()\n",
    "        if len(line) == 0:\n",
    "            del local_lines[idx]\n",
    "            continue\n",
    "        if idx + lines_in_group > len(local_lines):\n",
    "            res_proc_item = line\n",
    "        else:\n",
    "            second_line = local_lines[idx + 1].strip().lower()\n",
    "            if len(second_line) == 0:\n",
    "                del local_lines[idx + 1]\n",
    "                continue\n",
    "            res_proc_item = line + ' ' + second_line\n",
    "        rap_sentences.append(res_proc_item)\n",
    "        idx += lines_in_group\n",
    "    return rap_sentences\n",
    "\n",
    "# TODO how to have a deal with slang?\n",
    "def get_approx_sentences_from_lyrics(lyrics, local_processed_data):\n",
    "    couplets = re.split('\\[.*\\]' ,lyrics)\n",
    "    for couplet in couplets:\n",
    "        local_lines = couplet.split('\\n')\n",
    "        \n",
    "        if len(local_lines) == 1:\n",
    "            continue\n",
    "        lines_in_group = 2\n",
    "        rap_sentences = join_rap_sentences(local_lines, lines_in_group)\n",
    "        local_processed_data.extend(rap_sentences)\n",
    "\n",
    "def get_data_lyrics(substorage, topic_key):\n",
    "    local_processed_data = []\n",
    "    for lyrics in substorage.values():\n",
    "        approx_sentences = get_approx_sentences_from_lyrics(lyrics, local_processed_data)\n",
    "    local_processed_labels = [map_labels[topic_key]] * len(local_processed_data)\n",
    "    with open(f'tmp_{topic_key}.json', 'w') as f:\n",
    "        json.dump({'section' : local_processed_data}, f, indent=4, sort_keys=True)\n",
    "    return local_processed_data, local_processed_labels\n",
    "\n",
    "processed_data = []\n",
    "processed_labels = []\n",
    "for topic_key in map_labels.keys():\n",
    "    if topic_key == 'joke' or topic_key == 'usagov':\n",
    "        local_processed_data, local_processed_labels = get_data_rss(storage_to_process[topic_key], topic_key)\n",
    "        processed_data.extend(local_processed_data)\n",
    "        processed_labels.extend(local_processed_labels)\n",
    "    elif topic_key == 'rap':\n",
    "        local_processed_data, local_processed_labels = get_data_lyrics(storage_to_process[topic_key], topic_key)\n",
    "        processed_data.extend(local_processed_data)\n",
    "        processed_labels.extend(local_processed_labels)\n",
    "    else:\n",
    "        raise Exception(f'Unknown topic_key: {topic_key}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_data = pd.DataFrame({'text' : processed_data, 'label' : processed_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>antony j blinken, secretary of state</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>on behalf of the american people, i would like...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the united states and tanzania enjoy a longsta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>as tanzania also will soon celebrate  years of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i send my best wishes to the tanzanian people ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25165</th>\n",
       "      <td>i arrived in front of the dormitory \"yo, could...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25166</th>\n",
       "      <td>they showed me where it was for the moment i d...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25167</th>\n",
       "      <td>so, i came to her room and opened the door oh,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25168</th>\n",
       "      <td>a fella tongue-kissin' my girl in her mouth i ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25169</th>\n",
       "      <td>so please, listen to the message that i send d...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25170 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0                   antony j blinken, secretary of state      0\n",
       "1      on behalf of the american people, i would like...      0\n",
       "2      the united states and tanzania enjoy a longsta...      0\n",
       "3      as tanzania also will soon celebrate  years of...      0\n",
       "4      i send my best wishes to the tanzanian people ...      0\n",
       "...                                                  ...    ...\n",
       "25165  i arrived in front of the dormitory \"yo, could...      2\n",
       "25166  they showed me where it was for the moment i d...      2\n",
       "25167  so, i came to her room and opened the door oh,...      2\n",
       "25168  a fella tongue-kissin' my girl in her mouth i ...      2\n",
       "25169  so please, listen to the message that i send d...      2\n",
       "\n",
       "[25170 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    22494\n",
       "2     1375\n",
       "1     1301\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce number of samples of the largest class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_of_usagov = df_text_data[df_text_data['label'] == 0].index\n",
    "index_of_usagov\n",
    "remove_n = 18000\n",
    "drop_indices = np.random.choice(index_of_usagov, remove_n, replace=False)\n",
    "df_text_data_red = df_text_data.drop(drop_indices).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4494\n",
       "2    1375\n",
       "1    1301\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_data_red['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace transformers is coming!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = Dataset.from_pandas(df_text_data_red).remove_columns('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 7170\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"what is will smith's favourite band?\", 'label': 1}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset[5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28d8929e1df348c99b090ced30f96738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/650 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c53da97eb64a46d59619ce4f64ab2fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True).shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", num_train_epochs=50)\n",
    "metric = load_metric(\"bleurt\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    eval_dataset=tokenized_datasets,\n",
    "    compute_metrics=metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
